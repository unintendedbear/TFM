%---------------------------------------------------------------------
%
%                          Chapter 6
%
%---------------------------------------------------------------------
\renewcommand{\figurename}{Figure}
\chapter{Conclusions and future work}
\label{cap6:conclusions}

\begin{FraseCelebre}
\begin{Frase}
Whatever happens today, will change future events, create its own timeline, its own reality. The future pivots around you, here, now. So do good, for humanity, and for Earth.
\end{Frase}
\begin{Fuente}
11th Doctor to Amy. Doctor Who. \textit{Cold Blood}.
\end{Fuente}
\end{FraseCelebre}

Finally, this last Chapter is devoted to draw the conclusions over this research work, and names the papers that followed it. Also, as we want to continue researching in this topic, new objectives of future work are set out.

%-------------------------------------------------------------------
\section{Discussion}
%-------------------------------------------------------------------
\label{cap6:sec:discussion}

During the elaboration of this Master Thesis, a set of classification methods have been applied in order to perform a decision process inside a company, according to some predefined corporate security policies. This decision is focused on allowing or denying \ac{URL} access requests, but just considering previous decisions on similar requests, not by having specific rules in a White/Black List defined including those \ac{URL}s. Thus, the proposed method could allow or deny an access to a \ac{URL}, based in additional features rather than just the specific \ac{URL} string or its lexical properties. This would be very useful since new \ac{URL}s could be automatically `Whitelisted', or `Blacklisted', just depending on some of the connection parameters, such as the \texttt{content\_type} of the response or the \texttt{IP} of the client which makes the request.

To this aim, we have started from a big dataset (100000 patterns) about employees'  \ac{URL} sessions information (Section \ref{cap3:sec:log}), and considering a set of \ac{URL} access permissions (Section \ref{cap3:sec:rules}), we have composed a labelled dataset (more than 57000 labelled patterns, see Table \ref{tab_stats1}). Over that set of data, we have tested several classification methods, after some data balancing techniques have been applied. Then, the top five have been deeply proved over several training and test divisions (Section \ref{cap4:sec:traintest}), and with two methods: using sequential patterns (consecutive \ac{URL} accesses), and taking them in a randomly way.

We have performed experiments following a series of steps (see Figures \ref{fig:work_diagram_1} and \ref{fig:work_diagram_2}), and studying how the results changed and why.

The results shown at the beginning experiments, that classification accuracies are
between 95\% and 97\%, even when using the unbalanced datasets
(Section \ref{cap5:sec:exp}). However, they have been diminished
because of the possible loss of data that comes from performing an
undersampling (removing patterns) method; or taking the training and
the data sets in a sequential way from the main log file, due to the
fact that certain \ac{URL} requests can be made only at a certain
time.

Then, taking into account the way that the connections client/server passed through a proxy, we performed a removal of patterns that could be considered `duplicated' (see Section \ref{cap5:sec:exp:repurls}). And we still hade good accuracies (more than 96\%). Then, another step was taken forward and we tried to avoid repetition of the same core domain feature of
the \ac{URL}s in the same training/test file. This means a high loss of
information and so it is seen in the results. The last experiments shown accuracies not higher than
74\%.

Finally, a last set of experiments was conducted, in which we trained the classifiers including the \ac{TLD} feature of the \ac{URL}, but excluding the core domaind. Accuracies were again quite high, showing resultd between 94\% and 95\%, with low standard deviations.

Hence, after all the experiments done in different situations, we can conclude that the approach has been
successful and it would be a useful implementation on a tool for an actual enterprise.

Furthermore, we have found that for the data sets taken consecutively, the methods always classify worse the DENY labels, as they label them as ALLOW patterns. This is worth further study because it is the worst situation. It would be preferable to have a false positive in a DENY pattern, rather than a false negative and permit a request that is forbidden in the ISP.

% This is not a discussion, it's a description of the results. You
% should discuss why results are so bad or so good, why the methods
% used have happened to work or not, compare maybe with other methods,
% discuss whether these results are so because of preprocessing or the
% method... discussion is very important - JJ

%-------------------------------------------------------------------
\section{Scientific exploitation}
%-------------------------------------------------------------------
\label{cap6:sec:papers}

As a result of the work that has been done, three papers have been accepted (two of them also already presented, and one is to be presented in October 2014) in three conferences.

The paper titled ``\textit{MUSES: A corporate user-centric system which applies computational intelligence methods}'', by A.M. Mora, P. De las Cuevas, and J.J. Merelo, was accepted in a special track session of the \textbf{ACM SAC} conference, celebrated at Gyeongju, Korea, in March 2014. This special session was called \textbf{TRECK}, from \textit{Trust, Reputation, Evidence and other Collaboration Know-how}. The paper was presented at the conference by A.M. Mora.

\begin{description}
  \item[Abstract] This work presents the description of the architecture of a novel enterprise security system, still in development, which can prevent and deal with the security flaws derived from the users in a company. Thus, the Multiplatform Usable Endpoint Security system (MUSES) considers diverse factors such as the distribution of information, the type of accesses, the context where the users are, the category of users or the mix between personal and private data, among others. This system includes an event correlator and a risk and trust analysis engine to perform the decision process. MUSES follows a set of defined security rules, according to the enterprise security policies, but it is able to self-adapt the decisions and even create new security rules depending on the user behaviour, the specific device, and the situation or context. To this aim MUSES applies machine learning and computational intelligence techniques which can  also be used to predict potential unsafe or dangerous user's behaviour. 
\end{description}

The paper titled ``\textit{Enforcing Corporate Security Policies via Computational Intelligence Techniques}'', by A.M. Mora, P. De las Cuevas,  J.J. Merelo, S. Zamarripa, and Anna I. Esparcia-Alcázar, was accepted in a worshop at the \textbf{GECCO} conference, celebrated at Vancouver, Canada, in July 2014. This workshop was called \textbf{SecDef} - Workshop on \textit{Genetic and Evolutionary Computation in Defense, Security and Risk Management}. The paper was presented at the conference by J.J. Merelo and Anna I. Esparcia-Alcázar.

\begin{description}
  \item[Abstract] This paper presents an approach, based in a project in development, which combines Data Mining, Machine Learning and Computational Intelligence techniques, in order to create a user-centric and adaptable corporate security system. Thus, the system, named MUSES, will be able to analyse the user's behaviour (modelled as events) when interacting with the company's server, accessing to corporate assets, for instance. As a result of this analysis, and after the application of the aforementioned techniques, the Corporate Security Policies, and specifically, the Corporate Security Rules will be adapted to deal with new anomalous situations, or to better manage user's behaviour.
The work reviews the current state of the art in security issues resolution by means of these kind of methods. Then it describes the MUSES features in this respect and compares them with the existing approaches. 
\end{description}

The third paper derived from this research is titled ``\textit{Going a Step Beyond the Black and White Lists for URL Accesses in the Enterprise by means of Categorical Classifiers}'', by A.M. Mora, P. De las Cuevas, and J.J. Merelo. It was accepted at the \textbf{ECTA} conference, which is going to be held at Rome, Italy, in October 2014.

\begin{description}
  \item[Abstract] Corporate systems can be secured using an enormous quantity of methods, and the implementation of Black or White lists is among them. With these lists it is possible to restrict (or to allow) the users the execution of applications or the access to certain URLs, among others. This paper is focused on the latter option. It describes the whole processing of a set of data composed by URL sessions performed by the employees of a company; from the preprocessing stage, including labelling and data balancing processes, to the application of several classification algorithms. The aim is to define a method for automatically make a decision of allowing or denying future URL requests, considering a set of corporate security policies. Thus, this work goes a step beyond the usual black and white lists, since they can only control those URLs that are specifically included in them, but not by making decisions based in similarity (through classification techniques), or even in other variables of the session, as it is proposed here. The results show a set of classification methods which get very good classification percentages (95-97\%), and which infer some useful rules based in additional features (rather that just the URL string) related to the user's access. This led us to consider that this kind of tool would be very useful tool for an enterprise. 
\end{description}

%-------------------------------------------------------------------
\section{Future Work}
%-------------------------------------------------------------------
\label{cap6:sec:future}

Future lines of work include conducting a deeper set of experiments trying to test the generalisation power of the method, maybe considering bigger data divisions, bigger data sets (from a whole day or working day), or adding some kind of `noise'  to the dataset. Also, as the last experiments were succesful, more experiments will be conducted includind other features of the \ac{URL}. As found by reviewing the State of the Art (see Section \ref{cap2:sec:url}), the more features are extracted from the connection requests, the better results are obtained \citep{Ma_Url11}. Then, we will also extract additional information from the \ac{URL} string, than could be transformed into additional features that could be more discriminative than the current set. Moreover, a data process involving summarizing data about sessions (such as number of requests per client, or average time connection) will be also considered.

Thus, one of the future steps to follow will be to perform experiments with other two datasets which we have recently had access to: 

\begin{itemize}
  \item A log file provided in JSON format, which contains log entries from 12 days, and 5 million patterns. As can be found in \ac{CPAN} \citep{cpan_json}, there is a Perl module to directly work with this format, so is a great opportunity to continue testing.
  \item Another log file, in CSV format as the one processed during this research work, that is a subset of the previous data set and contains 1 million entries. This one colud be processed before the one in JSON format, given that we already have the implementation to preprocess CSV log files. 
\end{itemize}

Furthermore, considering the good classification results obtained, another next step could be the application of these methods in the real system from which data was gathered, counting with the opinion of expert \ac{CSO}s, in order to know the real value of the proposal.
The study of other classification methods could be another research
branch, along with the implementation of a Genetic Programming
approach, which could deal with the imbalance problem using a
modification of the cost associated with  misclassifying, could be done (as the authors did in \citep{cost_adjustment_07}).


% Variable local para emacs, para  que encuentre el fichero maestro de
% compilación y funcionen mejor algunas teclas rápidas de AucTeX
%%%
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../Tesis.tex"
%%% End:
