%---------------------------------------------------------------------
%
%                          Chapter 4
%
%---------------------------------------------------------------------
\renewcommand{\figurename}{Figure}
\chapter{Methodology}
\label{cap4:methodology}

\begin{FraseCelebre}
\begin{Frase}
DOCTOR: Oh, that's never going to work, is it?
CLARA: What's wrong? Do you think it's not done yet?
\end{Frase}
\begin{Fuente}
11th Doctor and Clara. Doctor Who. \textit{The Time of the Doctor}.
\end{Fuente}
\end{FraseCelebre}

This Chapter describes the two implementations that followed this research work. First, a fast analysis of both security policies and log data files required a language whith tools for an easy implementation of a parser for these kind of files. Section \ref{cap4:sec:Perl} describes why Perl was used for this task and how the first system (as a set of Perl scripts) was implemented. Then, for processing the data that has been analysed, and trying as many classification methods as possible, to see which ones work better with our data, we worked with Weka. Followed methodology with Weka is reported in section \ref{cap4:sec:Weka}. Finally, after a first round of experiments was done, before continue doing more experiments, and due to the high quantity of different files that needed to be created (separation of the original file in training and test files, as explained in Section \ref{cap4:sec:traintest}), a second implementation, not with Perl, was proposed. That implementation was required to be compatible with the use of Weka, in order to make things faster and automated. As Weka is developed in Java, the second part of the implementation was made in it, and the details are in Section \ref{cap4:sec:Java}. 

%-------------------------------------------------------------------
\section{First implementation, Perl}
%-------------------------------------------------------------------
\label{cap4:sec:Perl}
% first requirements, then language. What did you want? I wanted this
% and that. And Perl fulfilled those requirements. Don't post-justify
% - JJ 
% lo voy a intentar arreglar poniendo una introducción antes de empezar la sección
Perl was chosen for the initial development of the environment in which the data would be prepocessed and experiments would be developed. As a scripting language, it allows a faster application development than other languages such C or Java (although we have used Java in this work too, and in Section \ref{cap4:sec:Java} it is explained why), and faster performance too. In \citep{oreilly_perl07}, Tim O'Reilly, founder and \ac{CEO} of O'Reilly Media Inc \footnote{\url{http://www.oreilly.com/}}, and Ben Smith deeply describe the advantages of using Perl for every application, and also they present 11 actual cases in where the use of Perl enhanced their situations.

But the most important advantage of Perl is that is the best language for Information Processing. Thus, by means of its regular expressions, patterns of any type (depending on what the programmer is looking for) can be recognised, processed, stored, or studied. And even though Java uses regular expressions too, Perl scripting nature makes easier to use them. Furthermore, \ac{URL} study and HTML (body of free form text) processing is the ideal application for this Perl feature.

%-------------------------------------------------------------------
\subsection{Implementation}
%-------------------------------------------------------------------
\label{cap4:sec:Perl:subsec:implement}

We already presented in Sections \ref{cap3:sec:rules:subsec:drools} and \ref{cap3:sec:rules:subsec:logdata} the structure of the data to be processed and the desired output. As said before, Perl has the possibility of a quick parsing of files (bigger or small size) by using regular expressions. The next subsections describe how the parsing methods for Drool Rules and Squid Log Files were implemented. Then, it is explained how we ended with a Weka File (ARFF format) after a labelling process, and this way we accomplish the first objective of this work (see Section \ref{cap1:sec:intro:subsec:objectives}).

%-------------------------------------------------------------------
\subsubsection{Rules}
%-------------------------------------------------------------------
\label{cap4:sec:Perl:subsec:implement:subsec:rules}

As seen in Figure \ref{fig:drools_hash} (a), we are interested in both left and right side of the rule. In Table \ref{tab:drools_example} is depicted an example of the format of those sides of a normal rule, extracted from the policy ``Every intent of visualisation of a MP4 video streaming will be denied from the system''. More precisely, this example rule has four conditions, which have to occur in a log entry to finally apply a \textit{DENY} over it. At the same time, a condition has three fields: data type, relationship, and value.

To this end, we have applied the regular expressions defined in Figure \ref{fig:perl_rule_recognition} and obtained a hash with the format shown in Figure \ref{fig:drools_hash} (b).

\begin{table}[htpb]
\centering
{
\begin{tabular}{ | l | }
\hline
  /* Left Side of the Rule */ \\
  \hline
  squid:Squid(dif\_MCT == "video", bytes > 1000000, content\_type matches\\"*.application.*, url matches "*.p2p.* ) \\
  \hline
  /* Right Side of the Rule */ \\
  \hline
  PolicyDecisionPoint.deny(); \\
\hline
\end{tabular} 
}
\caption{Specification of a Rule in Drools, using also Squid syntax. \label{tab:drools_example}}
\end{table}

\begin{figure}[]
\begin{lstlisting}[basicstyle={\scriptsize\ttfamily},breaklines=true,language=Perl,numbers=left,stepnumber=1,numberstyle={\scriptsize},tabsize=3]
#Regular expressions to detect the left side of the rule: the conditions.
/\s*(.*)(==)"(.+)"/
/\s*(.+)([>|<|=])(\d+)/
/\s*(.+) (.+) "?\*\.(.+)\.\*"?/
#Regular expression to detect the action to apply to a log entry that meets the conditions.
/^\D+\.(.+)\(\)\;/
\end{lstlisting}
\caption{Perl regular expressions for obtaining information from the Rules.\label{fig:perl_rule_recognition}}
\end{figure}

%-------------------------------------------------------------------
\subsubsection{Log Data}
%-------------------------------------------------------------------
\label{cap4:sec:Perl:subsec:implement:subsec:log}

The first file to be processed was already mentioned in Section \ref{cap3:sec:log}, and it is a file with CSV extension, with the hundred thousand entries belonging to two work hours of the company. There is an example of one of the log entries in Figure \ref{fig:data_hash} (a), and some points have to be taken into account:

\begin{enumerate}
  \item The first row of the CSV file contains the fields that identify each one of the values in the subsequent rows.
  \item The values are separated by a semicolon.
  \item Some values are in between quotation marks, and others are not.
  \item The format of the data in the values is important when constructing a regular expression (for instance, the time).
  \item We are interested in extracting information about some values before storing them. First, ``content\_type'' (MIME types)\footnote{\url{http://www.w3.org/TR/html4/types.html\#h-6.7}} is usually formed by the \ac{MCT} (application, video, etc) and a subtype, and the parsing method should extract both (when present).
  \item Also, the lexical features of the \ac{URL} should be studied (see Section \ref{cap2:sec:url}). In this first research, we have worked with the core domains, due to many many rules in the supplied policies were focused in avoiding employees to enter dangerous sites.
\end{enumerate}

Hence, the created regular expressions for the creation of a hash with the data (shown in Figure \ref{fig:data_hash} (b)) are described in Figure \ref{fig:perl_log_recognition}.

\begin{figure}[]
\begin{lstlisting}[basicstyle={\scriptsize\ttfamily},breaklines=true,language=Perl,numbers=left,stepnumber=1,numberstyle={\scriptsize},tabsize=3]
#Cleaning quotation marks of the values which have them
/"(.+)"/
#Detecting and storing separately the Main Content Type and the subtype (in case there is a subtype)
/^(\w+-*\w+)[\/?]\w+/
#Detecting time format
/^\d{1,2}\:\d{2}\:\d{2}/
#Regular expressions related to URLs, they are already prepared for storing other lexical properties for its study, such as Top Level Domains, subdomains, and paths.
/^(ht|f)tps?:\/\/([\.\-\w]*)\.([\-\w]+)\.(\w+)\/[\/*\w*]*/
/^(ht|f)tps?:(\/\/)([\-\w]+)\.(\w+)\/[\/*\w*]*/
/^NONE\:\/\// #This has to be included in order to avoid carrying errors during the process.
\end{lstlisting}
\caption{Perl regular expressions for obtaining information from the Log File.\label{fig:perl_log_recognition}}
\end{figure}

%-------------------------------------------------------------------
\subsubsection{Labelling and creating the ARFF file}
%-------------------------------------------------------------------
\label{cap4:sec:Perl:subsec:implement:subsec:labels}

To finally fullfill the first objective, and now that we have two hashes with information, a method should be created in order to see if any log entry meets enough conditions to be labelled. It may happen, though, that for a certain connection, more than one rule could be applied. If those rules have the same action to take (all allow the action, or all deny it), there is no conflict, but it is the other way around. When a conflict appears between an allow and a deny label, we always apply the deny label over the allow, for security reasons.

Therefore, we make a verification for each entry, to see what rules can be apply to it and, at the end, we add a label as another field of the data hash. If there is no rule to apply to the connection in the log entry, the label field contains \textit{no\_label}.

Finally, data is needed to be in a format that Weka can manage, and hence we translate the obtained hash of labelled data to ARFF format, they way it is described in Figure \ref{fig:perl_arff}. Ir worths a comment that in Perl is very easy to obtain the rank of values for a categorical attribute, by following this process:

\begin{enumerate}
  \item Create a hash for each categorical attribute.
  \item Go over the values of the attribute, adding them to the hash as keys.
  \item If a value is repeated, the value of the pair ``key-value'' is incremented by 1.
  \item We obtain the rank of values simply accessing to the array of keys of the hash.
\end{enumerate}

Then, we fill the rest of the ARFF file by adding the same entries as were in the CSV, but taking care of the new features (including them in the same order as they are initialised in the attributes specification) and their format.

\begin{figure}[]
\begin{lstlisting}[basicstyle={\scriptsize\ttfamily},breaklines=true,language=Perl,numbers=left,stepnumber=1,numberstyle={\scriptsize},tabsize=3]
my $header=<<EOC;
\@RELATION logsUrl

EOC
  $header .= "\@ATTRIBUTE http_reply_code { ".join(",", @respuestas ).
    " }\n\@ATTRIBUTE http_method { ".join(",", @metodos).
      " }\n\@ATTRIBUTE duration_milliseconds REAL".
	"\n\@ATTRIBUTE content_type_MCT { ".join(",", @MCTs ).
	" }\n\@ATTRIBUTE content_type { ".join(",", @ctype ).
	" }\n\@ATTRIBUTE server_or_cache_address { ".join(",", @serveradd ).
	" }\n\@ATTRIBUTE time DATE \"HH:mm:ss\"".
	"\n\@ATTRIBUTE squid_hierarchy { ".join(",", @squidh ).
	" }\n\@ATTRIBUTE bytes REAL".
	"\n\@ATTRIBUTE url { ".join(",", @coredomains ).
	" }\n\@ATTRIBUTE client_address { ".join(",", @clientadd ).
	" }\n\@ATTRIBUTE label { ".join(",", @etiquetas ).
	" }\n\n\@DATA\n";

my $salida = "$header\n";
my $count = 0;
for my $r (@rows ) {
	$salida .= join(", ", @$r ).", ".$logentradas{$total_entradas[$count]}{"etiqueta"}."\n";
	$count++;
}
\end{lstlisting}
\caption{Perl code for transforming a hash of data into ARFF format, in order to be read by Weka.\label{fig:perl_arff}}
\end{figure}

%-------------------------------------------------------------------
\section{Weka}
%-------------------------------------------------------------------
\label{cap4:sec:Weka}

As said in Section \ref{cap3:sec:log}, the data used for this work is not only numerical or nominal, thus, only classification algorithms that support both types of data have been considered. Weka \citep{weka:site} is a collection of State-of-the-Art machine learning algorithms and data preprocessing tools that are key for data mining processes \citep{Frank2011}. With such a great number of possible algorithms to work with, we have conducted a preselection phase trying to choose those which would yield better results in the experiments. More specifically, we have focused on rule-based and decision-tree-based algorithms. 

In this way, a decision-tree algorithm is a group of conditions organised in a top-down recursive manner in a way that a class is assigned following a path of conditions, from the root of the tree to one of its leaves. Generally speaking, the possible classes to choose are mutually exclusive. Furthermore, these algorithms are also called ``divide-and-conquer'' algorithms. On the other hand, there are the ``separate-and-conquer'' algorithms, which work creating rules one at a time, then the instances covered by the created rule are removed and the next rule is generated from the remaining instances.

A reference to each Weka classifier can be found at \citep{Frank2011}. Below are described the top five techniques, obtained from the best results  of the experiments done in this stage, along with more specific bibliography. Naïve Bayes method \citep{Bayesian_Classifier_97} has been included as a baseline, normally used in text categorization problems. According to the results, the five selected classifiers are much better than this method. Deeper results are shown in Chapter \ref{cap5:results}.

\begin{description}
  \item[Naïve Bayes] It is the classification technique that we have used as a reference for either its simplicity and ease to understand. Its basis relies on the Bayes Theorem and the possibility of represent the relationship between two random variables as a Bayesian network \citep{rish2001empirical}. Then, by assigning values to the variables probabilities, the probabilities of the occurrences between them can be obtained. Thus, assuming that a set of attributes are independent one from another, and using the Bayes Theorem, patterns can be classified without the need of trees or rule creation, just by calculating probabilities.
   \item[J48] This classifier generates a pruned or unpruned C4.5 decision tree. Described for the first time in 1993 by \citep{Quinlan1993}, this machine learning method builds a decision tree selecting, for each node, the best attribute for splitting and create the next nodes. An attribute is selected as `the best' by evaluating the difference in entropy (information gain) resulting from choosing that attribute for splitting the data. In this way, the tree continues to grow till there are not attributes anymore for further splitting, meaning that the resulting nodes are instances of single classes. 
   \item[Random Forest] This manner of building a decision tree can be seen as a randomization of the previous C4.5 process. It was stated by \citep{Breiman2001} and consist of, instead of choosing `the best' attribute, the algorithm randomly chooses one between a group of attributes from the top ones. The size of this group is customizable in Weka.
   \item[REP Tree] Is another kind of decision tree, it means Reduced Error Pruning Tree. Originally stated by \citep{Quinlan1987}, this method builds a decision tree using information gain, like C4.5, and then prunes it using reduced-error pruning. That means that the training dataset is divided in two parts: one devoted to make the tree grow and another for pruning. For every subtree (not a class/leaf) in the tree, it is replaced by the best possible leaf in the pruning three and then it is tested with the test dataset if the made prune has improved the results. A deep analysis about this technique and its variants can be found in \citep{Elomaa2001}.
   \item[NNge] Nearest-Neighbor machine learning method of generating rules using non-nested generalised exemplars, i.e., the so called `hyperrectangles' for being multidimensional rectangular regions of attribute space \citep{Martin1995}. The NNge algorithm builds a ruleset from the creation of this hyperrectangles. They are non-nested (overlapping is not permitted), which means that the algorithm checks, when a proposed new hyperrectangle created from a new generalisation, if it has conflicts with any region of the attribute space. This is done in order to avoid that an example is covered by more than one rule (two or more).
   \item[PART] It comes from `partial' decision trees, for it builds its rule set from them \citep{Frank1998}. The way of generating a partial decision tree is a combination of the two aforementioned strategies ``divide-and-conquer'' and ``separate-and-conquer'', gaining then flexibility and speed. When a tree begins to grow, the node with lowest information gain is the chosen one for starting to expand. When a subtree is complete (it has reached its leaves), its substitution by a single leaf is considered. At the end the algorithm obtains a partial decision tree instead of a fully explored one, because the leafs with largest coverage become rules and some subtrees are thus discarded.
 \end{description} 

These methods are then deeply tested on the datasets that result from applying balancing methods over the initial one, and by makin a partition in training and test files. These processes are described in Section \ref{cap4:sec:traintest}.

%-------------------------------------------------------------------
\section{Second implementation, Java}
%-------------------------------------------------------------------
\label{cap4:sec:Java}

% same as above. Requirements, then solution - JJ
% fixed

Weka had been implemented in Java, and for that reason we started a second implementation with this language. As said in the previous Section \ref{cap4:sec:Perl}, the developing time with Java is higher, but in general the performance is better (see results in Section \ref{cap4:sec:Java:subsec:perlvsjava}), and we also have the possibility of developing a method for lauching the experiments and not doing them one by one. But if Perl had the advantage of being the best for preprocessing with the regular expressions, Java has the feature of directly process Weka output and, this way, obtaining reports and rankings with the best techniques.

%-------------------------------------------------------------------
\subsection{Implementation}
%-------------------------------------------------------------------
\label{cap4:sec:Java:subsec:implement}

The part of data preprocessing through regular expressions is not worth to be detailed, because they are the same as the ones in Perl (see Section \ref{cap4:sec:Perl:subsec:implement}), with little or no difference, and implemented with \texttt{Pattern} and \texttt{Matcher} classes. One difference with Perl is that here is not necessary to have both rules and log data hashes to compare, but here we have created three objects: \texttt{Condition.java}, \texttt{Rule.java}, and \texttt{LogEntry.java}, so the data is stored as an \texttt{ArrayList} of the correspondant objects. These objects has been described in Chapter \ref{cap3:data}.

Now, with regard to the direct management of Weka in Java, an \textit{ExperimentRunner.java} has been implemented and is called like is shown in Figure \ref{fig:java_exp}. There, it can be seen the syntax for calling the classifiers. Then, starting from the location where the ARFF file is, and the desired type of classifier, we create an Object of the type \texttt{Experiment} (from the weka.jar JAR) and start to customise it:

\begin{verbatim}
public static Double experimenter(String[] ARFF_File_input,
                                  String classifier_type)
                                  throws Exception {
	
System.out.println("Setting up...");
Experiment exp = new Experiment();
exp.setPropertyArray(new Classifier[0]);
exp.setUsePropertyIterator(true);
  
SplitEvaluator se  = new ClassifierSplitEvaluator();
Classifier sec = ((ClassifierSplitEvaluator) se).getClassifier();
\end{verbatim}

In the code above it is also specified that we are performing classification and not regression. After that, we specify that (for now) the classifier will be trained and tested by a crossvalidation process with 10 folds. This means that the dataset is divided in 10 equal parts and that the classifier will be trained an tested 10 times: each time it is trained with 9 of the 10 parts and the 10th is for testing. After the 10 generations, the results are calculated by averaging.

\begin{verbatim}
CrossValidationResultProducer cvrp = new CrossValidationResultProducer();
cvrp.setNumFolds(10);
cvrp.setSplitEvaluator(se);
      
PropertyNode[] propertyPath = new PropertyNode[2];
try {
 propertyPath[0] = new PropertyNode(se,
 		new PropertyDescriptor("splitEvaluator",
   	CrossValidationResultProducer.class),
   	CrossValidationResultProducer.class);
 propertyPath[1] = new PropertyNode( sec,
   	new PropertyDescriptor("classifier", se.getClass()), se.getClass());
} catch (IntrospectionException e) {
   	e.printStackTrace();
}	    
exp.setResultProducer(cvrp);
exp.setPropertyPath(propertyPath);
\end{verbatim}

The next step is to set the times that the experiment will be repeated.

\begin{verbatim}
exp.setRunLower(1);
exp.setRunUpper(1);
\end{verbatim}

And then it is time for setting up the classifier. First, the Weka method \texttt{Utils.splitOptions(classifier\_type)} takes the argument with which we called our ExperimentRunner, and given its syntax (see Figure \ref{fig:java_exp}), parses the options. With those options we build an Object of the type \texttt{Classifier}.

\begin{verbatim}
String[] options = Utils.splitOptions(classifier_type);
String classname = options[0];
options[0]  = "";
Classifier c = (Classifier) Utils.forName(Classifier.class,
                                          classname,
                                          options);
exp.setPropertyArray(new Classifier[]{c});
\end{verbatim}

What is left is to tell the Experimenter where to find the data set whose data we want to use for training and test the classifier. We set the data set with \texttt{exp.setDatasets(model)} and the configuration is almost finished.

\begin{verbatim}
DefaultListModel model = new DefaultListModel();
File file = new File(ARFF_File_input[1]);
model.addElement(file);
exp.setDatasets(model);
\end{verbatim}

All the results are displayed in a file with ARFF extension, like the one which contains the dataset to process. This way, here we set up the name of the output file (the same as the input one but with the suffix \textit{\_experimenter\_results}), and the location, the same as the input file.

\begin{verbatim}
InstancesResultListener irl = new InstancesResultListener();
String ARFF_File_name = ARFF_File_input[0].substring(0,
                                          ARFF_File_input[0].length()-5);
File ARFF_File_output = new File("/home/paloma/workspace/KRS_Prototype
                    /ARFF/"+ARFF_File_name+"_experimenter_results.arff");
irl.setOutputFile(ARFF_File_output);
exp.setResultListener(irl);
\end{verbatim}

Now, the configuration of the experiment is complete and we finish the method by initialising and running the experiment. But at the end, in Figure \ref{fig:java_exp_method} it can be seen how the results of the experiments are managed and stored in the set ARFF output file.

\begin{verbatim}
System.out.println("Initializing...");
exp.initialize();
System.out.println("Running...");
exp.runExperiment();
System.out.println("Finishing...");
exp.postProcess();
\end{verbatim}

Our method returns a \textit{double} variable with the percentage of correct predictions by the classifier or, in other words, its accuracy. Once we have the whole array of percentages, it is easy to choose which ones will be tested with the rest of the files, that have been obtained as detailed in Section \ref{cap4:sec:traintest}.

\begin{figure}[]
\begin{lstlisting}[basicstyle={\scriptsize\ttfamily},breaklines=true,language=Perl,numbers=left,stepnumber=1,numberstyle={\scriptsize},tabsize=3]
/* Definition of the Weka experiments. */
String[] experiments = new String[11];
experiments[0] = "NaiveBayes";
experiments[1] = "DecisionTable -X 1 -S \"weka.attributeSelection.BestFirst -D 1 -N 5\"";
experiments[2] = "JRip -F 3 -N 2.0 -O 2 -S 1";
experiments[3] = "OneR -B 6";
experiments[4] = "PART -M 2 -C 0.25 -Q 1";
experiments[5] = "ZeroR";
experiments[6] = "DecisionStump";
experiments[7] = "J48 -C 0.25 -M 2";
experiments[8] = "RandomForest -I 10 -K 0 -S 1 -num-slots 1";
experiments[9] = "RandomTree -K 0 -M 1.0 -V 0.001 -S 1";
experiments[10] = "REPTree -M 2 -V 0.001 -N 3 -S 1 -L -1 -I 0.0";

/* Array percentages for storing the accuracy obtained after each experiment. */
Double[] percentages = new Double[experiments.length]; 

/* Loop for launching the experiments. */	
int i;
for ( i = 0; i < experiments.length; i++) {
	System.out.println("Launching "+experiments[i]+"...");
	Double temp_percentage = ExperimentRunner.experimenter(ARFF_File_name, experiments[i]);
	if (!temp_percentage.isNaN()) {	
		percentages[i] = temp_percentage;
		System.out.println(percentages[i]);
	}
}
\end{lstlisting}
\caption[Java code for setting up the experiments with Weka.]{Java code for setting up the experiments with Weka and obtain an array of percentages of successfulness with each classifier.\label{fig:java_exp}}
\end{figure}

\begin{figure}[]
\begin{lstlisting}[basicstyle={\scriptsize\ttfamily},breaklines=true,language=Perl,numbers=left,stepnumber=1,numberstyle={\scriptsize},tabsize=3]
PairedTTester tester = new PairedCorrectedTTester();
Instances result = new Instances(new BufferedReader(new FileReader(irl.getOutputFile())));
tester.setInstances(result);
tester.setSortColumn(-1);
tester.setRunColumn(result.attribute("Key_Run").index());
tester.setFoldColumn(result.attribute("Key_Fold").index());
tester.setDatasetKeyColumns(
new Range(
   "" 
   + (result.attribute("Key_Dataset").index() + 1)));
tester.setResultsetKeyColumns(
new Range(
   "" 
   + (result.attribute("Key_Scheme").index() + 1)
   + ","
   + (result.attribute("Key_Scheme_options").index() + 1)
   + ","
   + (result.attribute("Key_Scheme_version_ID").index() + 1)));
tester.setResultMatrix(new ResultMatrixPlainText());
tester.setDisplayedResultsets(null);       
tester.setSignificanceLevel(0.05);
tester.setShowStdDevs(true);

/* Fill result matrix (but discarding the output) */
tester.multiResultsetFull(0, result.attribute("Percent_correct").index());
ResultMatrix matrix = tester.getResultMatrix();
return matrix.getMean(0, 0);
\end{lstlisting}
\caption{Java code for obtaining the output in an ARFF file as well as returning a double with the accuracy percentage.\label{fig:java_exp_method}}
\end{figure}

%-------------------------------------------------------------------
\subsection{Perl performance vs. Java performance}
%-------------------------------------------------------------------
\label{cap4:sec:Java:subsec:perlvsjava}

There are two implementations, in both Perl and Java, and each one has its advantages and disadvantages. In this subsection, there is a study of the time spent for each language in executing the tasks of parsing the data (Rules, first, and then the Log File), and label the data and storing the results in Weka format. The results can be found in Table \ref{tab:perl_vs_java}. They are completely expected because the strength of Perl is that of extracting data, and because of that it spends 15 seconds less than Java performing this task. On the other hand, Java is much more efficient in labelling the data and storing the data once labelled. Actually, that difference is of 24 seconds with respect to Perl.

It is expected to have a quicker performing by developing in Java, but instead of chosing only one, we have just demonstrated that the best performance can be obtained by using both languages, for different tasks.

\begin{table}[htpb]
\centering 
{\small
\begin{tabular}{l|l|l|}
\cline{2-3}
& \multicolumn{2}{ |c| }{Average time (sec)}\\ 
\hline
\multicolumn{1}{ |l| }{Action performed} & Perl & Java \\ 
\hline
\multicolumn{1}{ |l| }{Parsing Rules and Data} & 2.156 & 16.663 \\ 
\hline
\multicolumn{1}{ |l| }{Labelling and Creating ARFF format File} & 74.355 & 50.816 \\ 
\hline
\end{tabular}
}
\caption{Comparison of the performance time while preprocessing the information between Perl and Java.{\label{tab:perl_vs_java}}}
\end{table}

%-------------------------------------------------------------------
\section{File partitioning}
%-------------------------------------------------------------------
\label{cap4:sec:traintest}

At this point, we have explained how we reached the first objective we set on the Section \ref{cap1:sec:intro:subsec:objectives} and for accomplish the second, there is a series of different files the should be created, in order to test the classifiers and see how can we obtain the best results. The initial Log File, after being labelled, presents a total of 57502 labelled patterns, which 38972 patterns are allowed and 18530 are denied. We observed that more than half of the initial amount of patterns are labelled, and that the ratio is 2:1 in allows to denies. Hence, after a first set of experiments we obtained the results of Table \ref{tabresults_todos}. From these results, we took the top five classifiers and then prove them with the rest of the files.

The 2:1 ratio means that the data is unbalanced, and that balancing techniques may affect in the results when applied. Also, as it may be recalled, the data tested with the classifiers was used both for training and test by crossvalidation. For the results to be more accurate, the next step is to build separate training and test files to assure that the data used for training is not used again for testing, and vice versa.

The way they have been generated is pretty trivial, but what matters is that each time that some file is created randomly, me made 3 generations of files and the same experiments with every one of them. Then, if an 80\% training file was created by taking the log entries generating a random number, we repeated the process two times more and at the end we had three sets of training and test files, and three group of five experiments for each one. After that, the means and standard deviations were calculated. The statistics of the number of patterns and how the labels are distributed can be found at Tables \ref{tab_stats1}, \ref{tab_stats2}, and \ref{tab_stats3}. These are tables that contain the number of samples (patterns, or log entries) that were obteined after each separation on training and test files. So these tables show that the implemented method would effectively split them. As it is shown, percentages are obtained, although not exact, almost equal on random generations, and equal on consecutive.

This being said, the work has been distributed as follows:

\begin{description}
  \item[Step 1] Figure \ref{fig:work_diagram_1} shows the first step: experiments were made with files of unbalanced and balanced data (with both undersampling and oversampling balancing techniques), and then again but with those files separated in training and test files, both randomly and consecutively taken. Two different separations were made: 80\% for training, \%20 for testing, and 90\% for tranining, 10\% for testing. The reason for generating these files also keeping the sequence of the initial data and not only randomly, is because the requested \ac{URL}s, or the time of the requesting, can be related.
  \item[Step 2] After having analised the log of connecting patterns, we studied the field \textit{squid\_hierarchy} and saw that had two possible values: \texttt{DIRECT} or \texttt{DEFAULT\_PARENT}. The Squid FAQ reference \citep{squid_logs}, and the Squid wiki \citep{squid_wiki} explain that, as a proxy, the connections are made, firstly to the Squid proxy, and then, if appropriate, the request continues to another server. These connections are registered in Squid the same way, with the same fields, with the exception of the client and server \ac{IP} addresses. From the point of view of classification, if one of these two entries happens to be in the training file, and the other in the testing file, it would mean that the second would be well classified because of all the attribute values that both have in common. However, this means also that the good percentages that we obtained may not be real, but faked. That is why the second step is about removing entries that we called ``repeated'' (in the explained sense). After the removal, a new set of files was created.
  \item[Step 3] Another `issue' related to classification accuracies, was taking care of the repeated core domains. Like with the connections, if a Log Entry with a certain core domain, goes to a training file, and another one (even if the rest of the fields are different) goes to the test file, it may affect the results. As a result, a whole new set of files were created and studied.
  \item[Step 4] As it will be described in Section \ref{cap5:sec:rule} of next Chapter, we saw that the rules created by the classifiers were too focused on the \ac{URL} core domain feature. Thus, we wanted to add another set of experiments, represented as \textit{step 4} in Figure \ref{fig:work_diagram_1}, because we did the experiments again with the original file, but including as a feature only the \ac{TLD} of the \ac{URL}, and not the core domain. This was the last set of files created for this research work.
\end{description}

On steps 2 and 3, the separation 60\% for tranining, 40\% for testing was added when possible, for studying the generalization of the classifiers. In the next chapter we will present the results of the experiments, and in Chapter \ref{cap6:conclusions}, the derived conclussions.

\begin{figure}[htb]
\centering
\subfloat[Training and Test files, random]{
\small
\begin{tabular}[t]{|l|c|c|c|c|c|c|}
\hline
File &             Total & Allow & Deny & \% of Original & \%Allows & \%Denies \\
\hline
Original &            57502 & 38972 & 18530 & 100 & 67.78 & 32.22 \\
\hline
Training 80\% &  45837 & 31026 & 14811 & 79.71 & 67.69 & 32.31 \\
\hline
Test 20\% &           11665 & 7946 & 3719 & 20.29 & 68.12 & 31.88 \\
\hline
Training 90\% &  51764 & 35102 & 16662 & 90.02 & 67.81 & 32.19 \\
\hline
Test 10\% &           5738 & 3870 & 1868 & 9.98 & 67.45 & 32.55 \\
\hline
\end{tabular}
}

\subfloat[Training and Test files, consecutive]{
\small
\begin{tabular}[t]{|l|c|c|c|c|c|c|}
\hline
File &             Total & Allow & Deny & \% of Original & \%Allows & \%Denies \\
\hline
Original &            57502 & 38972 & 18530 & 100 & 67.78 & 32.22 \\
\hline
Training 80\% &  46002 & 31178 & 14824 & 80.00 & 67.78 & 32.22 \\
\hline
Test 20\% &           11500 & 7794 & 3706 & 20.00 & 67.77 & 32.23 \\
\hline
Training 90\% &  51752 & 35075 & 16677 & 90.00 & 67.78 & 32.22 \\
\hline
Test 10\% &           5750 & 3897 & 1853 & 10.00 & 67.77 & 32.23 \\
\hline
\end{tabular}
}
\caption[Statistics of the files created from the initial file of unbalanced data.]{\label{tab_stats1} Statistics of the files created from the initial file of unbalanced data. It can be seen that after the partition, the ratio between allows and denies remains the same. (a) Files generated by randomly taking patterns from the original file. (b) Files generated by consecutively taking patterns from the original file.}
\end{figure}

\begin{figure}[htb]
\centering
\subfloat[Training and Test files, random]{
\small
\begin{tabular}[t]{|l|c|c|c|c|c|c|}
\hline
File &             Total & Allow & Deny & \% of Original & \%Allows & \%Denies \\
\hline
Original &            38081 & 19569 & 18512 & 100 & 51.39 & 48.61 \\
\hline
Training 80\% &  30486 & 15664 & 14822 & 80.06 & 51.38 & 48.62 \\
\hline
Test 20\% &           7594 & 3905 & 3689 & 19.94 & 51.42 & 48.58 \\
\hline
Training 90\% &  34184 & 17536 & 16648 & 89.77 & 51.30 & 48.70 \\
\hline
Test 10\% &           3896 & 2033 & 1863 & 10.23 & 52.18 & 47.82 \\
\hline
\end{tabular}
}

\subfloat[Training and Test files, consecutive]{
\small
\begin{tabular}[t]{|l|c|c|c|c|c|c|}
\hline
File &             Total & Allow & Deny & \% of Original & \%Allows & \%Denies \\
\hline
Original &            38081 & 19569 & 18512 & 100 & 51.38 & 48.61 \\
\hline
Training 80\% &  30465 & 15655 & 14810 & 80.00 & 51.39 & 48.61 \\
\hline
Test 20\% &           7615 & 3914 & 3701 & 20.00 & 51.40 & 48.61 \\
\hline
Training 90\% &  34273 & 17612 & 16661 & 90.00 & 51.39 & 48.61 \\
\hline
Test 10\% &           3807 & 1957 & 1850 & 10.00 & 51.41 & 48.59 \\
\hline
\end{tabular}
}
\caption[Statistics of the files created after the \textbf{undersampling} technique was applied.]{\label{tab_stats2} Statistics of the files created after the \textbf{undersampling} technique was applied. It can be seen that after the partition, the ratio between allows and denies remains the same, and they are balanced now. (a) Files generated by randomly taking patterns from the original file. (b) Files generated by consecutively taking patterns from the original file.}
\end{figure}

\begin{figure}[htb]
\centering
\subfloat[Training and Test files, random]{
\small
\begin{tabular}[t]{|l|c|c|c|c|c|c|}
\hline
File &             Total & Allow & Deny & \% of Original & \%Allows & \%Denies \\
\hline
Original &            76032 & 38972 & 37060 & 100 & 51.26 & 48.74 \\
\hline
Training 80\% &  60748 & 31104 & 29644 & 79.90 & 51.20 & 48.80 \\
\hline
Test 20\% &           15283 & 7867 & 7416 & 20.10 & 51.48 & 48.52 \\
\hline
Training 90\% &  68477 & 35047 & 33430 & 90.06 & 51.18 & 48.82 \\
\hline
Test 10\% &           7554 & 3924 & 3630 & 9.94 & 51.95 & 48.05 \\
\hline
\end{tabular}
}

\subfloat[Training and Test files, consecutive]{
\small
\begin{tabular}[t]{|l|c|c|c|c|c|c|}
\hline
File &             Total & Allow & Deny & \% of Original & \%Allows & \%Denies \\
\hline
Original &            76032 & 38972 & 37060 & 100 & 51.26 & 48.74 \\
\hline
Training 80\% &  60826 & 31178 & 29648 & 80.00 & 51.26 & 48.74 \\
\hline
Test 20\% &           15205 & 7793 & 7412 & 20.00 & 51.25 & 48.75 \\
\hline
Training 90\% &  68429 & 35075 & 33354 & 90.00 & 51.26 & 48.74 \\
\hline
Test 10\% &           7602 & 3896 & 3706 & 10.00 & 51.25 & 48.75 \\
\hline
\end{tabular}
}
\caption[Statistics of the files created after the \textbf{oversampling} technique was applied.]{\label{tab_stats3} Statistics of the files created after the \textbf{oversampling} technique was applied. It can be seen that after the partition, the ratio between allows and denies remains the same, and they are balanced now. (a) Files generated by randomly taking patterns from the original file. (b) Files generated by consecutively taking patterns from the original file.}
\end{figure}

\begin{figure}[htb]
\centering
\subfloat[Training and Test files, random, 1st generation]{
\small
\begin{tabular}[t]{|l|c|c|c|c|c|c|}
\hline
File &             Total & Allow & Deny & \% of Original & \%Allows & \%Denies \\
\hline
Original &            38619 & 26318 & 12301 & 100 & 68.15 & 31.85 \\
\hline
Training 80\% &  30080 & 20261 & 9819 & 77.89 & 67.36 & 32.64 \\
\hline
Test 20\% &           8539 & 6057 & 2482 & 22.11 & 70.93 & 29.07 \\
\hline
Training 90\% &  33917 & 22981 & 10936 & 87.82 & 67.76 & 32.24 \\
\hline
Test 10\% &           4702 & 3337 & 1365 & 12.18 & 70.97 & 29.03 \\
\hline
Training 60\% &  21771 & 14519 & 7252 & 56.37 & 66.69 & 33.31 \\
\hline
Test 40\% &           16848 & 11799 & 5049 & 43.63 & 70.03 & 29.97 \\
\hline
\end{tabular}
}

\subfloat[Training and Test files, random, 2nd generation]{
\small
\begin{tabular}[t]{|l|c|c|c|c|c|c|}
\hline
File &             Total & Allow & Deny & \% of Original & \%Allows & \%Denies \\
\hline
Original &            38619 & 26318 & 12301 & 100 & 68.15 & 31.85 \\
\hline
Training 80\% &  30034 & 20394 & 9640 & 77.77 & 67.90 & 32.10 \\
\hline
Test 20\% &           8585 & 5924 & 2661 & 22.23 & 69.00 & 31.00 \\
\hline
Training 90\% &  33839 & 22873 & 10966 & 87.62 & 67.59 & 32.41 \\
\hline
Test 10\% &           4780 & 3345 & 1335 & 12.38 & 69.98 & 27.93 \\
\hline
Training 60\% &  22447 & 15213 & 7234 & 58.12 & 67.77 & 32.23 \\
\hline
Test 40\% &           16172 & 11105 & 5067 & 41.88 & 68.67 & 31.33 \\
\hline
\end{tabular}
}

\subfloat[Training and Test files, random, 3rd generation]{
\small
\begin{tabular}[t]{|l|c|c|c|c|c|c|}
\hline
File &             Total & Allow & Deny & \% of Original & \%Allows & \%Denies \\
\hline
Original &            38619 & 26318 & 12301 & 100 & 68.15 & 31.85 \\
\hline
Training 80\% &  30096 & 20198 & 9898 & 77.93 & 67.11 & 32.89 \\
\hline
Test 20\% &           8523 & 6120 & 2403 & 22.07 & 71.81 & 28.19 \\
\hline
Training 90\% &  33854 & 22793 & 11061 & 87.66 & 67.33 & 32.67 \\
\hline
Test 10\% &           4765 & 3525 & 1240 & 12.34 & 73.98 & 26.02 \\
\hline
Training 60\% &  22390 & 15026 & 7364 & 57.98 & 67.11 & 32.89 \\
\hline
Test 40\% &           16229 & 11292 & 4937 & 42.02 & 69.58 & 30.42 \\
\hline
\end{tabular}
}

\subfloat[Training and Test files, consecutive.]{
\small
\begin{tabular}[t]{|l|c|c|c|c|c|c|}
\hline
File &             Total & Allow & Deny & \% of Original & \%Allows & \%Denies \\
\hline
Original &            38619 & 26318 & 12301 & 100 & 68.15 & 31.85 \\
\hline
Training 80\% &  30895 & 21054 & 9841 & 80.00 & 68.15 & 31.85 \\
\hline
Test 20\% &           7723 & 5263 & 2460 & 20.00 & 68.15 & 31.85 \\
\hline
Training 90\% &           34757 & 23686 & 11071 & 90.00 & 68.15 & 31.85 \\
\hline
Test 10\% &           3861 & 2631 & 1230 & 10.00 & 68.14 & 31.86 \\
\hline
Training 60\% &           23172 & 15791 & 7381 & 60.00 & 68.15 & 31.85 \\
\hline
Test 40\% &           15446 & 10526 & 4920 & 40.00 & 68.15 & 31.85 \\
\hline
\end{tabular}
}
\caption[Statistics of the files created after the duplicated connections were removed.]{\label{tab_stats3} Statistics of the files created after the duplicated connections were removed. It can be seen that after the partitions, the ratio between allows and denies remains the same, unbalanced. (a), (b), and (c) Three generations of files generated by randomly taking patterns from the original file. (d) Files generated by consecutively taking patterns from the original file.}
\end{figure}

\figuraEx{Vectorial/work_diagram_1.pdf}{width=0.8\textwidth}{fig:work_diagram_1}%
{This Figure shows that experiments were made with files of unbalanced and balanced data (with both undersampling and oversampling balancing techniques), and again but with those files parted in training and test files, both randomly and consecutively taken.}{Diagram showing the first and fourth groups of performed experiments.}

\figuraEx{Vectorial/work_diagram_2.pdf}{width=0.8\textwidth}{fig:work_diagram_2}%
{Two main files were created. First, requests between the client and the server that passed through the proxy (duplicated, from the point of view of classification training), were reduced. Then, when parting the main file into training and test files, it was made avoiding repetition of core domains in both files.}{Diagram showing the second and third groups of performed experiments.}


% Variable local para emacs, para  que encuentre el fichero maestro de
% compilación y funcionen mejor algunas teclas rápidas de AucTeX
%%%
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../Tesis.tex"
%%% End:

% Only talking about implementation, you should tell how you are doing
% the experimental design - JJ
