%---------------------------------------------------------------------
%
%                          Chapter 5
%
%---------------------------------------------------------------------
\renewcommand{\figurename}{Figure}
\chapter{Results}
\label{cap5:results}

\begin{FraseCelebre}
\begin{Frase}
You know the Doctor. You understand him. You will predict his actions.
\end{Frase}
\begin{Fuente}
DALEK. Doctor Who. \textit{The Parting of the Ways}.
\end{Fuente}
\end{FraseCelebre}

%-------------------------------------------------------------------
\section{Experiment results}
%-------------------------------------------------------------------
\label{cap5:sec:exp}

The initial group of conducted experiments was presented in Section \ref{cap4:sec:Weka}, and the top five was formed by the classifiers: J48, Random Forest, REP Tree, NNge, and PART. These five classifiers were training and tested with both training and test files, as explained in Section \ref{cap4:sec:traintest}. First results can be found in Table \ref{tabresults_nobalan} \footnote{All result tables are publicly accessible through the following link: \url{https://drive.google.com/folderview?id=0B0Rvxfh8NwQRMi1RYU9uT3FmQzg&usp=sharing}}.

\begin{table*}[htpb]
\centering
{\small
\begin{tabular}{|l|l|l|l|l|}
\cline{2-5}
\multicolumn{1}{l|}{} & \multicolumn{2}{c|}{80\% Training - 20\% Test} & \multicolumn{2}{c|}{90\% Training - 10\% Test} \\ 
\cline{2-5}
\multicolumn{1}{l|}{} & Random (mean) & Sequential & Random (mean) & Sequential \\ 
\hline
J48 & 97.56 $\pm$ 0.20 & 88.48 & 97.70 $\pm$ 0.15 & 82.28 \\ 
\cline{1-1}
Random Forest & 97.68 $\pm$ 0.20 & 89.77 & 97.63 $\pm$ 0.13 & 82.59 \\ 
\cline{1-1}
REP Tree & 97.47 $\pm$ 0.11 & 88.34 & 97.57 $\pm$ 0.01 & 83.20 \\ 
\cline{1-1}
NNge & 97.23 $\pm$ 0.10 & 84.41 & 97.38 $\pm$ 0.36 & 80.34 \\ 
\cline{1-1}
PART & 97.06 $\pm$ 0.19 & 89.11 & 97.40 $\pm$ 0.16 & 84.17 \\ 
\hline
\end{tabular}
}
\caption{\label{tabresults_nobalan} Percentage of correctly classified patterns for non-balanced data}
\end{table*}

As it can be seen, all five methods achieved a high performance classifying in the right way the test dataset. Also, these results are not like this by chance, as shown by a low standard deviation. Although it was expected that the results from the 90\%-10\% division were slightly better, in the future a more aggressive division will be executed so the methods can be really proved with much less training data.

What matters to the results of the experiments made with the sequential data, they are worse than the obtained from the random data, but still they are good ($>$ 85\%). This is due to the occurrence of new patterns from a certain time (maybe there are some requests that are made just at one specific time in a day, or in settled days), and then there is no sufficient similarity between the training data and the classifying of the test data set may fail. The loss of 5 to 6 points in the results of the 90\%-10\% division is the first unexpected or unlogical result of the experiments, but they also reinforce the previous theory.

The technique that lightly stands out over the others is \textit{Random Forest}, being the best in almost every case, even in the experiments with the most complex sequential divisions. However, if we focus on the standard deviation, \textit{REP Tree} is the chosen one, as its results present robustness. 

For its part, results obtained from unbalanced data are shown in Table \ref{tabresults_balan}. Again the corresponding to the random partitions come from the mean of three blocks of experiments, and so are specified the standard deviations. The Table illustrates two segments of results, obtained from the undersampled data and from the oversampled data. For each one, the 90\%-10\% and 80\%-20\% divisions were also made.


\begin{figure}[htb]
\centering
\subfloat[80\% for training, \%20 for testing]{
\small
\begin{tabular}{|l|l|l|l|l|}
\cline{2-5}
\multicolumn{1}{l|}{} & \multicolumn{4}{c|}{80\% Training - 20\% Test} \\ 
\cline{2-5}
\multicolumn{1}{l|}{} & \multicolumn{2}{c|}{Undersampling} & \multicolumn{2}{c|}{Oversampling} \\ 
\cline{2-5}
\multicolumn{1}{l|}{} & Rand (mean) & Sequential & Rand (mean) & Sequential \\ 
\hline
J48 & 97.05 $\pm$ 0.25 & 84.29 & 97.40 $\pm$ 0.03 & 85.66 \\ 
\cline{1-1}
Random Forest & 96.61 $\pm$ 0.17 & 88.59 & 97.16 $\pm$ 0.19 & 89.03 \\ 
\cline{1-1}
REP Tree & 96.52 $\pm$ 0.13 & 85.54 & 97.13 $\pm$ 0.25 & 85.41 \\ 
\cline{1-1}
NNge & 96.56 $\pm$ 0.42 & 85.28 & 96.90 $\pm$ 0.28 & 83.46 \\ 
\cline{1-1}
PART & 96.19 $\pm$ 0.14 & 85.16 & 96.82 $\pm$ 0.09 & 84.50 \\ 
\hline
\end{tabular}
}

\subfloat[90\% for tranining, 10\% for testing]{
\small
\begin{tabular}{|l|l|l|l|l|}
\cline{2-5}
\multicolumn{1}{l|}{} & \multicolumn{4}{c|}{90\% Training - 10\% Test} \\ 
\cline{2-5}
\multicolumn{1}{l|}{} & \multicolumn{2}{c|}{Undersampling} & \multicolumn{2}{c|}{Oversampling} \\ 
\cline{2-5}
\multicolumn{1}{l|}{} & Rand (mean) & Sequential & Rand (mean) & Sequential \\ 
\hline
J48 & 96.85 $\pm$ 0.35 & 76.44 & 97.37 $\pm$ 0.06 & 74.24 \\ 
\cline{1-1}
Random Forest & 96.99 $\pm$ 0.13 & 79.98 & 97.25 $\pm$ 0.33 & 81.33 \\ 
\cline{1-1}
REP Tree & 96.55 $\pm$ 0.10 & 77.65 & 97.14 $\pm$ 0.09 & 76.81 \\ 
\cline{1-1}
NNge & 96.33 $\pm$ 0.05 & 81.93 & 96.91 $\pm$ 0.06 & 78.73 \\ 
\cline{1-1}
PART & 96.09 $\pm$ 0.10 & 79.70 & 96.68 $\pm$ 0.11 & 78.16 \\ 
\hline
\end{tabular}
}
\caption[Percentage of correctly classified patterns for balanced data, after step 1.]{Percentage of correctly classified patterns for balanced data (undersampling and oversampling). (a) 80\% for training, \%20 for testing. (b) 90\% for tranining, 10\% for testing. \label{tabresults_balan}}
\end{figure}

\begin{description}
  \item[Applying Undersampling] In comparison with those results from Table \ref{tabresults_nobalan}, these go down one point (in the case of randomly made divisions) to six points (sequential divisions). The reason why this happens is that when randomly removing ALLOW patterns, we are really losing information, i. e. key patterns that could be decisive in a good classification of a certain set of test patterns. 
  \item[Applying Oversampling] Here we have duplicated the DENY patterns so their number could be up to that of the ALLOW patterns. However, it does not work as well as in other approaches which uses numerical computations for creating the new patterns to include in the minority class. Consequently, the results have been decreased.
\end{description}

It is noticeable in both cases that taking the data in a sequential way, instead of randomly, lower the results. It is clear that due to the fact that performing undersampling some patterns are lost while in the case of oversampling they all remain, \textit{undersampling results} are better. Then, in this case the algorithm with best performance is \textit{J48}, though \textit{Random Forest} follows its results very closely in random datasets processing, and \textit{REP Tree}, which is better than the rest when working with sequential data. Nevertheless, generally speaking and given the aforementioned reasons, performing data balancing methods yields worse results.

Furthermore, we have found that for the data sets taken consecutively, the methods always classify worse the DENY labels, as they label them as ALLOW patterns. This is worth further study because it is the worst situation. It would be preferable to have a false positive in a DENY pattern, rather than a false negative and permit a request that is forbidden in the ISP.

%-------------------------------------------------------------------
\subsection{Removal of duplicated requests}
%-------------------------------------------------------------------
\label{cap5:sec:exp:repurls}

After performing the step 2 that is depicted in Figure \ref{fig:work_diagram_2}, a whole new ranking of classification performance was created, in orde to see if the new situation has some influence on the results. However, Table  \ref{tab:gobalrank_repurls} shows that the best results are from the same classifiers, with slightly difference when performing oversampling, where \textit{Random Tree} was better than \textit{REP Tree} but less than one point.

\begin{table}[htpb]
\centering
{\small
\begin{tabular}{|l|l|l|l|}
\cline{2-4}
\multicolumn{1}{l|}{} & Unbalanced & Undersampling & Oversampling \\
\hline
Naïve Bayes & 92.30 & 90.77 $\pm$ 0.06 & 91.77 \\
\cline{1-1}
Conjunctive Rule & 73.31 & 59.53 $\pm$ 0.15 & 60.02 \\
\cline{1-1}
Decision Table & 95.21 & 93.73 $\pm$ 0.18 & 90.29 \\
\cline{1-1}
DTNB & 95.55 & 94.75 $\pm$ 0.07 & 95.65 \\
\cline{1-1}
JRip & 92.95 & 89.64 $\pm$ 0.32 & 92.47 \\
\cline{1-1}
NNge & \textbf{97.18} & \textbf{96.33 $\pm$ 0.14} & \textbf{98.76} \\
\cline{1-1}
One R & 94.86 & 93.53 $\pm$ 0.04 & 93.70 \\
\cline{1-1}
PART & \textbf{97.41} & \textbf{96.32 $\pm$ 0.10} & \textbf{97.54} \\
\cline{1-1}
Ridor & 89.21 & 86.19 $\pm$ 0.79 & 89.87 \\
\cline{1-1}
Zero R & 68.14 & 51.68 $\pm$ 0.17 & 51.26 \\
\cline{1-1}
AD Tree & 85.23 & 78.01 $\pm$ 0.10 & 77.68 \\
\cline{1-1}
Decision Stump & 73.31 & 59.53 $\pm$ 0.15 & 60.02 \\
\cline{1-1}
J48 & \textbf{97.37} & \textbf{96.65 $\pm$ 0.04} & \textbf{98.00} \\
\cline{1-1}
LAD Tree & 86.87 & 80.24 $\pm$ 0.04 & 79.97 \\
\cline{1-1}
Random Forest & \textbf{97.57} & \textbf{96.75 $\pm$ 0.05} & \textbf{98.84} \\
\cline{1-1}
Random Tree & 96.11 & 95.45 $\pm$ 0.66 & \textbf{98.35} \\
\cline{1-1}
REP Tree & \textbf{97.32} & \textbf{96.38 $\pm$ 0.07} & 97.67 \\
\hline
\end{tabular}
}
\caption[Global classification methods ranking after step 2.]{\label{tab:gobalrank_repurls} Results of all tested classification methods on unbalanced and balanced data, after the repeated requests have been removed. The best ones are marked in boldface.}
\end{table}

The process is again the same, with the difference that now it has been included \textit{Naïve Bayes} as a reference in all the groups of experiments. The results are displayed in Tables \ref{tab:repurl_unb_traintest} (a) and (b). With regard to the 60\% - 40\% partition of part (b), . 

We can see that the results are slightly worse than the ones obtained in step 1, but they are still good, and definitely better than Naïve Bayes, our result reference. It is not a surprise that, again, results for files with the patterns taken consecutively lower significantly. As previously, this is because the possible loss of information. Best results are obtained by both \textit{Random Forest} and \textit{REP Tree} classifiers, with a 96\% of accuracy.

\begin{figure}[htb]
\centering
\subfloat[80\% for training, \%20 for testing, and 90\% for tranining, 10\% for testing]{
\small
\begin{tabular}{|l|l|l|l|l|}
\cline{2-5}
\multicolumn{1}{l|}{} & \multicolumn{2}{c|}{80\% Training - 20\% Test} & \multicolumn{2}{c|}{90\% Training - 10\% Test} \\ 
\cline{2-5}
\multicolumn{1}{l|}{} & Random (mean) & Sequential & Random (mean) & Sequential \\ 
\hline
Naïve Bayes & 93.01 $\pm$ 0.32 & 82.61 & 93.09 $\pm$ 0.91 & 83.04 \\ 
\cline{1-1}
Random Forest & 96.97 $\pm$ 0.47 & 91.03 & 96.79 $\pm$ 0.97 & 80.60 \\ 
\cline{1-1}
J48 & 96.90 $\pm$ 0.26 & 87.78 & 96.50 $\pm$ 1.00 & 84.49 \\ 
\cline{1-1}
NNge & 96.21 $\pm$ 0.28 & 81.17 & 96.11 $\pm$ 1.13 & 81.92 \\ 
\cline{1-1}
REP Tree & 96.97 $\pm$ 0.40 & 87.75 & 96.62 $\pm$ 0.87 & 85.57 \\ 
\cline{1-1}
PART & 96.84 $\pm$ 0.18 & 86.68 & 96.55 $\pm$ 0.87 & 83.61 \\ 
\hline
\end{tabular}
}

\subfloat[60\% for tranining, 40\% for testing]{
\small
\begin{tabular}{|l|c|}
\cline{2-2}
\multicolumn{1}{l|}{} & \multicolumn{1}{c|}{60\% Training - 40\% Test} \\ 
\cline{2-2}
\multicolumn{1}{l|}{} & Random (mean) \\ 
\hline
Naïve Bayes & 92.76 $\pm$ 1.34 \\ 
\cline{1-1}
Random Forest & 96.45 $\pm$ 0.67 \\ 
\cline{1-1}
J48 & 96.44 $\pm$ 0.19 \\ 
\cline{1-1}
NNge & 95.98 $\pm$ 0.34 \\ 
\cline{1-1}
REP Tree & 96.41 $\pm$ 0.27 \\ 
\cline{1-1}
PART & 96.05 $\pm$ 0.45 \\ 
\hline
\end{tabular}
}
\caption[Correctly classified patterns for non-balanced data after the step 2.]{Percentage of correctly classified patterns for non-balanced data, after the step 2. (a) 80\% for training, \%20 for testing, and 90\% for tranining, 10\% for testing. (b) 60\% for tranining, 40\% for testing. \label{tab:repurl_unb_traintest}}
\end{figure}

%-------------------------------------------------------------------
\subsection{Enhancing the creation of training and test files}
%-------------------------------------------------------------------
\label{cap5:sec:exp:coredomains}

The last step taken in this research was to try to better separate the patterns from the initial file. Thus, we tried to avoid possible classification errors by assuring that entries with the same core domain end in the same file (either for training o testing) (see Figure \ref{fig:work_diagram_2}).

It must be pointed out that, for the moment, there are no generations of training and test files with the patterns taken consecutively. The reason for this can be seen also in Table \ref{tab:coredom_stats}: taking into account this way of separating the data, when a certain log entry happens to be placed in the training, or testing, file, all the others which share the same \ac{URL} core domain, will follow to the same file, and therefore is impossible to reach the desired percentages.

Furthermore, taking a look at the results in Table \ref{tab:coredom_unb_traintest}, they are by far the worst we obtained. It seems that a lot of information is lost by performing this step. Figures \ref{fig:results_8020}, \ref{fig:results_9010}, and \ref{fig:results_6040} show a comparison between the tested classifiers during this process. It is shown that the best classifiers are \textit{J48} and \textit{REP Tree} (between 70.69\& and 74.72\% of accuracy).

\begin{figure}[htb]
\centering
\subfloat[Training and Test files, random, 1st generation]{
\small
\begin{tabular}[t]{|l|c|c|c|c|}
\hline
File &             Total & Allow & Deny & \% of Original \\
\hline
Original &            38619 & 26318 & 12301 & 100 \\
\hline
Training 80\% &  30466 & 20169 & 10297 & 78.79  \\
\hline
Test 20\% &           8153 & 6149 & 2004 & 21.11  \\
\hline
Training 90\% &  29964 & 20454 & 9510 & 77.59  \\
\hline
Test 10\% &           8655 & 5864 & 2791 & 22.41  \\
\hline
Training 60\% &  19272 & 13705 & 5567 & 49.90  \\
\hline
Test 40\% &           19347 & 12613 & 6734 & 50.10 \\
\hline
\end{tabular}
}

\subfloat[Training and Test files, random, 2nd generation]{
\small
\begin{tabular}[t]{|l|c|c|c|c|}
\hline
File &             Total & Allow & Deny & \% of Original \\
\hline
Original &            38619 & 26318 & 12301 & 100 \\
\hline
Training 80\% &  28368 & 18206 & 10162 & 73.46 \\
\hline
Test 20\% &           10251 & 8112 & 2139 & 26.54 \\
\hline
Training 90\% &  31463 & 20776 & 10687 & 81.47 \\
\hline
Test 10\% &           7156 & 5542 & 1614 & 18.53 \\
\hline
Training 60\% &  21945 & 14814 & 7131 & 56.82 \\
\hline
Test 40\% &           16674 & 11504 & 5170 & 43.18 \\
\hline
\end{tabular}
}

\subfloat[Training and Test files, random, 3rd generation]{
\small
\begin{tabular}[t]{|l|c|c|c|c|}
\hline
File &             Total & Allow & Deny & \% of Original \\
\hline
Original &            38619 & 26318 & 12301 & 100 \\
\hline
Training 80\% &  31068 & 21862 & 9206 & 80.45 \\
\hline
Test 20\% &           7551 & 4456 & 3095 & 19.55 \\
\hline
Training 90\% &  32034 & 21119 & 10915 & 82.95 \\
\hline
Test 10\% &           6585 & 5199 & 1386 & 17.05 \\
\hline
Training 60\% &  23337 & 14416 & 8921 & 60.43 \\
\hline
Test 40\% &           15282 & 11902 & 3380 & 39.57 \\
\hline
\end{tabular}
}
\caption[Statistics of the files created after the duplicated connections were removed.]{\label{tab:coredom_stats} Statistics of the files created after the duplicated connections were removed. It can be seen that after the partitions, the ratio between allows and denies remains the same, unbalanced. (a), (b), and (c) Three generations of files generated by randomly taking patterns from the original file. (d) Files generated by consecutively taking patterns from the original file.}
\end{figure}


\begin{table}[htpb]
\centering
{\small
\begin{tabular}{|l|l|l|l|}
\hline
\multicolumn{1}{|l|}{Percentage training \& test} & \multicolumn{1}{c|}{80\% - 20\%} & \multicolumn{1}{c|}{90\% - 10\%} & \multicolumn{1}{c|}{60\% - 40\%} \\ 
\hline
Naïve Bayes & 48.67 $\pm$ 11.70 & 44.99 $\pm$ 18.59 & 44.58 $\pm$ 10.73 \\ 
\cline{1-1}
Random Forest & 67.48 $\pm$ 9.61 & 74.21 $\pm$ 6.01 & 70.29 $\pm$ 5.06 \\ 
\cline{1-1}
J48 & 71.19 $\pm$ 10.71 & 74.72 $\pm$ 6.08 & 70.69 $\pm$ 6.51 \\ 
\cline{1-1}
NNge & 58.19 $\pm$ 4.18 & 65.82 $\pm$ 5.70 & 53.77 $\pm$ 5.59 \\ 
\cline{1-1}
REP Tree & 71.19 $\pm$ 10.71 & 74.72 $\pm$ 6.08 & 70.69 $\pm$ 6.51 \\ 
\cline{1-1}
PART & 68.37 $\pm$ 13.36 & 67.52 $\pm$ 9.60 & 64.23 $\pm$ 5.82 \\ 
\hline
\end{tabular}
}
\caption[Correctly classified patterns for non-balanced data after the step 3.]{\label{tab:coredom_unb_traintest} Percentage of correctly classified patterns for non-balanced data after the step 3. There are three different sets of files: 80\% for training, \%20 for testing; 90\% for tranining, 10\% for testing; and 60\% for tranining, 40\% for testing.}
\end{table}

\figuraEx{Vectorial/trtst8020.pdf}{width=1.\textwidth}{fig:results_8020}%
{It is shown that the best classifiers are \textit{J48} and
  \textit{REP Tree} (71.19\% of accuracy).}{Comparison between
  classifiers, with 80\% for training, \%20 for testing, in
  performance at step 3.} % Don't use "it is shown", ahórrate
                          % pasiva. Di "This figure shows" - JJ

\figuraEx{Vectorial/trtst9010.pdf}{width=1.\textwidth}{fig:results_9010}%
{It is shown that the best classifiers are \textit{J48} and
  \textit{REP Tree} (74.72\% of accuracy)}{Comparison between
  classifiers, with 90\% for training, \%10 for testing, in
  performance at step 3.} %describe qué hace la figura, no pongas
                          %conclusiones aquí. Que sea mejor o peor
                          %debe decidirlo quien lo ve.

% Errores múltiples con las figuras
% * usas capturas de pantalla. usa R y no Excel. Si usas Excel (o lo
% que sea), exporta a HTML y te salen los JPG monísimos en el
% directorio de ficheros.
% * Los ejes deben de empezar siempre en 0. 
% * No estoy muy seguro de que debas representar las incorrectamente
% clasificadas. Son 100 - las bien clasificadas, es información
% espúrea - JJ
% * Siempre hay que representar la desviación estándar - JJ

\figuraEx{Vectorial/trtst6040.pdf}{width=1.\textwidth}{fig:results_6040}%
{It is shown that the best classifiers are \textit{J48} and \textit{REP Tree} (70.69\% of accuracy)}{Comparison between classifiers, with 60\% for training, \%40 for testing, in performance at step 3.}

%-------------------------------------------------------------------
\section{Rules obtained, a study case}
%-------------------------------------------------------------------
\label{cap5:sec:rule}

With regard to the obtained rules/trees, we want to remark that the majority are based on the \ac{URL} in order to discriminate between the two classes, however we also found several ones which consider variables/features different of this to make the decision. For instance:\\

\begin{verbatim}
IF server_or_cache_address = "90.84.53.17" 
THEN DENY

IF server_or_cache_address = "173.194.78.103" 
THEN ALLOW

IF content_type = 
 "application/vnd.google.safebrowsing-update" 
THEN DENY

IF server_or_cache_address = "173.194.78.94" 
AND content_type_MCT = "text"
AND content_type = "text/html"
AND http_reply_code = "200"
AND bytes > 772
THEN ALLOW

IF server_or_cache_address = "173.194.34.225"
AND http_method = "GET"
AND duration_milliseconds > 52
THEN ALLOW

IF server_or_cache_address = "90.84.53.49"
AND time <= 33758000
THEN ALLOW
\end{verbatim}

These rules are the most interesting for our purposes, since they are somehow independent of the \ac{URL} to which the client requests to access. Thus, it would be potentially possible to allow or deny the access to unknown \ac{URL}s just taking into account some parameters of the session.

Of course, some of these features depend on the session itself, i.e. they will be computed after the session is over, but the idea in that case would be 'to refine' somehow the existing set of \ac{URL}s in the White List.
Thus, when a client requests access to a Whitelisted \ac{URL}, this will be allow, but after the session is over, and depending on the obtained values, and on one of these classifiers, the \ac{URL} could be labelled as DENIED for further requests.
This could be a useful decision-aid tool for the CSO in a company, for instance.
In case that the features considered in the rule can be known in advance, such as \texttt{http\_method}, or \texttt{server\_or\_cache\_address}, for example, the decision could be made in real-time, and thus, a granted \ac{URL} (Whitelisted) could be DENIED or the other way round.


%The tree-based methods also yield several useful branches in this sense, but they have not been plotted here because of the difficulty for showing/visualizing them properly.



% Variable local para emacs, para  que encuentre el fichero maestro de
% compilación y funcionen mejor algunas teclas rápidas de AucTeX
%%%
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../Tesis.tex"
%%% End:

% Debes añadir una sección donde digas qué es lo que funciona mejor y
% por qué - JJ
